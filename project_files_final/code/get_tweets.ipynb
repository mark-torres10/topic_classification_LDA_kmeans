{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import csv\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import tweepy\n",
    "import re \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mark/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/mark/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download nltk packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    \"\"\"\n",
    "        Function: removes punctuation from a line of text. Also removes emojis\n",
    "    \"\"\"\n",
    "\n",
    "    # remove emojis\n",
    "    text = re.sub(r'\\\\U00\\S+', ' ', text)\n",
    "    text = re.sub(r'\\\\u\\S+', ' ', text)\n",
    "    \n",
    "    # remove weird text with \\\\n\n",
    "    text = re.sub(r'\\\\n\\S+', ' ', text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]',' ', text)\n",
    "\n",
    "    # return clean text\n",
    "    return text\n",
    "\n",
    "def remove_links(tweet):\n",
    "    \"\"\"\n",
    "        Function: takes a string and removes web links from it\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove links\n",
    "    tweet = re.sub(r'http\\S+', '', tweet) # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) # rempve bitly links\n",
    "    tweet = re.sub(r'bitly/\\S+', '', tweet) # rempve bitly links\n",
    "    \n",
    "    # return cleaned text\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    \"\"\"\n",
    "        Function: takes a string and removes retweet and @user information\n",
    "    \"\"\"\n",
    "    # remove user references\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove retweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove tweeted at\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def word_lemmatizer(tweet):\n",
    "    \"\"\"\n",
    "        Function: takes a string, tokenizes it (separates it into distinct words) \n",
    "        and lemmatizes it (simplifies to root form)\n",
    "        \n",
    "    \"\"\"\n",
    "    # initializer lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # tokenize, split sentence into words\n",
    "    word_list = nltk.word_tokenize(tweet)\n",
    "    \n",
    "    # lemmatize list of words and join\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(word) for word in word_list])\n",
    "    \n",
    "    # return output\n",
    "    return lemmatized_output\n",
    "\n",
    "def eliminate_stopwords(tweet):\n",
    "    \n",
    "    \"\"\"\n",
    "        Function: takes a tweet, separates it by whitespace delimiter, then takes out stopwords (e.g., \"the\", \"and\")\n",
    "    \"\"\"\n",
    "    # define stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # get cleaned tweet\n",
    "    cleaned_tweet = [w for w in tweet.split() if not w in stop_words]\n",
    "    \n",
    "    # re-join tweet\n",
    "    cleaned_tweet = ' '.join(cleaned_tweet)\n",
    "    \n",
    "    # return cleaned tweet\n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### import data ######\n",
    "data_dir = \"../data/labeled_tweets_climateChange.csv\"\n",
    "\n",
    "# keep tweets in array:\n",
    "tweet_text = []\n",
    "\n",
    "with open(data_dir) as csv_file:\n",
    "    readCSV = csv.reader(csv_file, delimiter = ',')\n",
    "\n",
    "    # iterate by row\n",
    "    for row in readCSV:\n",
    "        # append to array that holds tweets\n",
    "        tweet_text.append(row[1])\n",
    "        \n",
    "# take out first entry, which just has \"tweet\" \n",
    "tweet_text = tweet_text[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stopwords list\n",
    "stopwords_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lowercase\n",
    "lowercase_tweets = [text.lower() for text in tweet_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove links\n",
    "tweet_no_link = [remove_links(tweet) for tweet in lowercase_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove users\n",
    "tweet_no_users = [remove_users(tweet) for tweet in tweet_no_link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "tweet_no_punctuation = [remove_punctuations(tweet) for tweet in tweet_no_users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize words\n",
    "tweets_lemmatized = [word_lemmatizer(tweet) for tweet in tweet_no_punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "tweets_no_stopwords = [eliminate_stopwords(tweet) for tweet in tweets_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as csv\n",
    "with open(\"../data/cleaned_tweets.csv\", 'w', newline = '') as csv_file:\n",
    "    wr = csv.writer(csv_file, delimiter = '\\n')\n",
    "    clean_tweets = [tweets_no_stopwords]\n",
    "    for tweet in clean_tweets:\n",
    "        wr.writerow(tweet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
